{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper-parameter Optimization\n",
    "\n",
    "In this notebook hyper-parameters of XGBoost are optimized for [HIGGS](https://archive.ics.uci.edu/ml/datasets/HIGGS) dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env CUDA_DEVICE_ORDER=PCI_BUS_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, [scikit-optimize](https://scikit-optimize.github.io/) (skopt) library is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skopt import Optimizer\n",
    "\n",
    "from skopt.learning import GaussianProcessRegressor\n",
    "from skopt.learning.gaussian_process.kernels import Matern, RBF, WhiteKernel\n",
    "\n",
    "from skopt.learning import RandomForestRegressor\n",
    "\n",
    "from skopt.acquisition import gaussian_ei as acq_func"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data\n",
    "\n",
    "The original dataset is quite large, thus, in order to save time, only a portion of it is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT='/mnt/mlhep2018/datasets/'\n",
    "\n",
    "SAMPLE_LIMIT=100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "\n",
    "data = np.genfromtxt(\n",
    "    osp.join(DATA_ROOT, 'HIGGS.csv'),\n",
    "    dtype='float32',\n",
    "    delimiter=',',\n",
    "    max_rows=SAMPLE_LIMIT\n",
    ")\n",
    "\n",
    "X, y = data[:, 1:], data[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### again to save training time only 10% of the loaded data is used for training.\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.9)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X, y, test_size=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Max depth tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For illustration purposes we perform Baysian Optimization on 1 parameter: `learning rate`.\n",
    "\n",
    "Learning rate in XGBoost can vary dramatically by orders of magnitude, thus, we use *logarithmic scale*.\n",
    "\n",
    "The goal is to maximize `ROC AUC` (minimize 1 - `ROC AUC`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_function_1(log_learning_rate, X_train=X_train, y_train=y_train, X_score=X_test, y_score=y_test):\n",
    "    from xgboost import XGBClassifier\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    \n",
    "    xgb = XGBClassifier(\n",
    "        max_depth=3, \n",
    "        n_estimators=50,\n",
    "        learning_rate=np.exp(log_learning_rate),\n",
    "        objective='binary:logistic',\n",
    "        n_jobs=20,\n",
    "        reg_lambda=0.0\n",
    "    )\n",
    "\n",
    "    xgb.fit(X_train, y_train)\n",
    "    predictions = xgb.predict_proba(X_score)[:, 1]\n",
    "    \n",
    "    return 1 - roc_auc_score(y_score, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### learning rate <- [1.0e-3, 1]\n",
    "dimensions_1 = [\n",
    "    (np.log(1.0e-3), 0)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "target_function_1(0.0, X_train, y_train, X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### utility function to inspect status of BO.\n",
    "\n",
    "def plot_bo(bo, suggestion=None, value=None):\n",
    "    a, b = bo.space.bounds[0]\n",
    "    \n",
    "    ### getting the latest model\n",
    "    model = bo.models[-1]\n",
    "    \n",
    "    xs = np.linspace(a, b, num=100)\n",
    "    x_model = bo.space.transform(xs.reshape(-1, 1).tolist())\n",
    "    \n",
    "    mean, std = model.predict(x_model, return_std=True)\n",
    "    \n",
    "    plt.subplots(nrows=1, ncols=2, figsize=(12, 4))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.scatter(\n",
    "        np.array(bo.Xi)[:, 0],\n",
    "        np.array(bo.yi),\n",
    "        color='red',\n",
    "        label='observations'\n",
    "    )\n",
    "    if suggestion is not None:\n",
    "        plt.scatter([suggestion], value, color='blue', label='suggestion')\n",
    "    \n",
    "    plt.plot(xs, mean, color='green', label='model')\n",
    "    plt.fill_between(xs, mean - 1.96 * std, mean + 1.96 * std, alpha=0.1, color='green')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    acq = acq_func(x_model, model, np.min(bo.yi))\n",
    "    plt.plot(xs, acq, label='Expected Improvement')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### another utility function\n",
    "def cum_min(xs):\n",
    "    result = np.zeros_like(xs)\n",
    "    cmin = xs[0]\n",
    "    \n",
    "    result[0] = xs[0]\n",
    "    \n",
    "    for i in range(1, xs.shape[0]):\n",
    "        if cmin > xs[i]:\n",
    "            cmin = xs[i]\n",
    "\n",
    "        result[i] = cmin\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "\n",
    "### plots progress of BO over time.\n",
    "\n",
    "def plot_convergence(bo):\n",
    "    display.clear_output(wait=True)\n",
    "    values = np.array(bo.yi)\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(cum_min(values), label='minimal discovered')\n",
    "    plt.scatter(np.arange(len(bo.yi)), bo.yi, label='observations')\n",
    "    plt.xlabel('step', fontsize=14)\n",
    "    plt.ylabel('loss', fontsize=14)\n",
    "    \n",
    "    plt.legend(loc='upper right', fontsize=18)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian optimization with Gaussian Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### create an instance of Baysian Optimizer\n",
    "bo_gp_1 = Optimizer(\n",
    "    ### telling optimizer boundaries for each parameter\n",
    "    dimensions=dimensions_1,\n",
    "    \n",
    "    ### setting regressor\n",
    "    base_estimator=GaussianProcessRegressor(\n",
    "        kernel=RBF(length_scale_bounds=[1.0e-6, 1.0e+6]) + \\\n",
    "            WhiteKernel(noise_level=1.0e-5, noise_level_bounds=[1.0e-6, 1.0e-2]),\n",
    "    ),\n",
    "    n_initial_points=2,\n",
    "    acq_func='EI',   \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in range(20):\n",
    "    x = bo_gp_1.ask()\n",
    "    f = target_function_1(x[0], X_train, y_train, X_val, y_val)\n",
    "\n",
    "    if len(bo_gp_1.models) > 0:\n",
    "        plot_bo(bo_gp_1, suggestion=x, value=f)\n",
    "    \n",
    "    bo_gp_1.tell(x, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_convergence(bo_gp_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best = np.argmin(bo_gp_1.yi)\n",
    "best_parameters = bo_gp_1.Xi[best]\n",
    "\n",
    "print('Best log learning rate: %.2e' % best_parameters[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_auc = 1 - target_function_1(best_parameters[0], X_train, y_train, X_test, y_test)\n",
    "\n",
    "print('Best ROC AUC: %.3lf' % best_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian optimization with Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### the same but with random forest\n",
    "\n",
    "bo_rf_1 = Optimizer(\n",
    "    dimensions=dimensions_1,\n",
    "    base_estimator=RandomForestRegressor(\n",
    "        n_estimators=100,\n",
    "        min_variance=1.0e-6\n",
    "    ),\n",
    "    n_initial_points=3,\n",
    "    acq_func='EI',   \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in range(20):\n",
    "    x = bo_rf_1.ask()\n",
    "    f = target_function_1(x[0])\n",
    "\n",
    "    if len(bo_rf_1.models) > 0:\n",
    "        plot_bo(bo_rf_1, suggestion=x, value=f)\n",
    "    \n",
    "    bo_rf_1.tell(x, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_convergence(bo_rf_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best = np.argmin(bo_rf_1.yi)\n",
    "best_parameters = bo_rf_1.Xi[best]\n",
    "\n",
    "print('Best log learning rate: %.2e' % best_parameters[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_auc = 1 - target_function_1(best_parameters[0], X_train, y_train, X_test, y_test)\n",
    "\n",
    "print('Best ROC AUC: %.3lf' % best_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning all important hyper-parameters simultaneously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_function_4(params, X_train, y_train, X_score, y_score):\n",
    "    \"\"\"\n",
    "    params - array with 4 values: [max tree depth, number of estimators, log learning rate, reg_lambda]\n",
    "    X_train, y_train - training dataset;\n",
    "    X_score, y_score - dataset for evaluating quality of the classifier;\n",
    "    \n",
    "    Returns 1 - ROC AUC evaluated on `X_score` and `y_score`. \n",
    "    \"\"\"\n",
    "    max_depth, n_estimators, log_learning_rate, reg_lambda = params\n",
    "\n",
    "    max_depth = int(np.ceil(max_depth))\n",
    "    n_estimators = int(np.ceil(n_estimators))\n",
    "    \n",
    "    learning_rate = np.exp(log_learning_rate)\n",
    "\n",
    "    from xgboost import XGBClassifier\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    \n",
    "    xgb = XGBClassifier(\n",
    "        max_depth=max_depth,\n",
    "        n_estimators=n_estimators,\n",
    "        learning_rate=learning_rate,\n",
    "        reg_lambda=reg_lambda,\n",
    "        objective='binary:logistic',\n",
    "        n_jobs=4\n",
    "    )\n",
    "    xgb.fit(X_train, y_train)\n",
    "    predictions = xgb.predict_proba(X_score)[:, 1]\n",
    "    \n",
    "    return 1 - roc_auc_score(y_score, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_best(bo):\n",
    "    best_result_index = np.argmin(bo.yi)\n",
    "    best_parameters = bo.Xi[best_result_index]\n",
    "\n",
    "    max_depth, n_estimators, log_learning_rate, reg_lambda = best_parameters\n",
    "    \n",
    "    print(\n",
    "        'Best model:\\n\\nmax_depth=%d\\nn_estimators=%d\\nlearning_rate=%.1e\\nreg_lambda=%.3lf' % (\n",
    "            int(np.ceil(max_depth)),\n",
    "            int(np.ceil(n_estimators)),\n",
    "            np.exp(log_learning_rate),\n",
    "            reg_lambda\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    best_auc = target_function_4(best_parameters, X_train, y_train, X_test, y_test)\n",
    "    print('Best ROC AUC: %.3lf' % (1 - best_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimensions_4 =[\n",
    "    ### max_depth\n",
    "    (1.0, 20.0),\n",
    "    \n",
    "    ### n_estimators\n",
    "    (1.0, 500.0),\n",
    "    \n",
    "    ### log_learning rate\n",
    "    (np.log(1.0e-3), np.log(1.0)),\n",
    "    \n",
    "    ### l2 reg\n",
    "    (0.0, 1.0)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bayesian Optimization with Gaussian Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bo_gp_4 = Optimizer(\n",
    "    dimensions=dimensions_4,\n",
    "    base_estimator=GaussianProcessRegressor(\n",
    "        kernel=RBF(length_scale_bounds=[1.0e-3, 1.0e+3]) +\\\n",
    "            WhiteKernel(noise_level=1.0e-5, noise_level_bounds=[1.0e-6, 1.0e-2])\n",
    "    ),\n",
    "    n_initial_points=5,\n",
    "    acq_func='EI',   \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(50):\n",
    "    x = bo_gp_4.ask()\n",
    "    f = target_function_4(x, X_train, y_train, X_val, y_val)\n",
    "    \n",
    "    bo_gp_4.tell(x, f)\n",
    "    \n",
    "    plot_convergence(bo_gp_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_best(bo_gp_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The same with Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bo_rf_4 = Optimizer(\n",
    "    dimensions=dimensions_4,\n",
    "    base_estimator=RandomForestRegressor(\n",
    "        n_estimators=100, n_jobs=4, min_variance=1.0e-6\n",
    "    ),\n",
    "    n_initial_points=5,\n",
    "    acq_func='EI',   \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(50):\n",
    "    x = bo_rf_4.ask()\n",
    "    f = target_function_4(x, X_train, y_train, X_val, y_val)\n",
    "    \n",
    "    bo_rf_4.tell(x, f)\n",
    "    \n",
    "    plot_convergence(bo_rf_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_best(bo_rf_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structural Optimization\n",
    "\n",
    "\n",
    "Sometimes it is preferable to have a good classifier with a particular configuration.\n",
    "\n",
    "For example, smaller ensemble is preferable to large ones (given the same quality), since small ensemble are faster to compute. One way to find preferable classifiers is to introduce a penalty to the target function:\n",
    "\n",
    "$$\\mathcal{L} = \\mathrm{quality\\;metric} + \\mathrm{penalty}$$\n",
    "\n",
    "\n",
    "For example, number of CPU operations per prediction can introduced as a penalty: \n",
    "$$\\mathcal{L} = \\mathrm{ROC\\; AUC} + C \\cdot \\mathrm{tree\\;depth} \\cdot \\mathrm{ensemble\\;size}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_function_4_struct(params, X_train, y_train, X_score, y_score):\n",
    "    \"\"\"\n",
    "    params - array with 4 values: [max tree depth, number of estimators, log learning rate, reg_lambda]\n",
    "    X_train, y_train - training dataset;\n",
    "    X_score, y_score - dataset for evaluating quality of the classifier;\n",
    "    \n",
    "    Returns 1 - ROC AUC evaluated on `X_score` and `y_score`. \n",
    "    \"\"\"\n",
    "    max_depth, n_estimators, log_learning_rate, reg_lambda = params\n",
    "\n",
    "    max_depth = int(np.ceil(max_depth))\n",
    "    n_estimators = int(np.ceil(n_estimators))\n",
    "    \n",
    "    learning_rate = np.exp(log_learning_rate)\n",
    "\n",
    "    from xgboost import XGBClassifier\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    \n",
    "    xgb = XGBClassifier(\n",
    "        max_depth=max_depth,\n",
    "        n_estimators=n_estimators,\n",
    "        learning_rate=learning_rate,\n",
    "        reg_lambda=reg_lambda,\n",
    "        objective='binary:logistic',\n",
    "        n_jobs=4\n",
    "    )\n",
    "    xgb.fit(X_train, y_train)\n",
    "    predictions = xgb.predict_proba(X_score)[:, 1]\n",
    "    \n",
    "    score = 1 - roc_auc_score(y_score, predictions)\n",
    "    \n",
    "    penalty = 5.0e-5 * n_estimators * max_depth\n",
    "    \n",
    "    return score + penalty "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bo_gp_4_struct = Optimizer(\n",
    "    dimensions=dimensions_4,\n",
    "    base_estimator=GaussianProcessRegressor(\n",
    "        kernel=RBF(length_scale_bounds=[1.0e-3, 1.0e+3]) +\\\n",
    "            WhiteKernel(noise_level=1.0e-5, noise_level_bounds=[1.0e-6, 1.0e-2])\n",
    "    ),\n",
    "    n_initial_points=5,\n",
    "    acq_func='EI',   \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    x = bo_gp_4_struct.ask()\n",
    "    f = target_function_4_struct(x, X_train, y_train, X_val, y_val)\n",
    "    \n",
    "    bo_gp_4_struct.tell(x, f)\n",
    "    \n",
    "    plot_convergence(bo_gp_4_struct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_best(bo_gp_4_struct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your turn\n",
    "\n",
    "Try to optimize neural network architecture for the HIGGS dataset.\n",
    "\n",
    "Your neural network **must** have less than $10^4$ parameters!\n",
    "\n",
    "Target metric: $\\mathrm{ROC\\;AUC} \\to \\max$\n",
    "\n",
    "**Normal problem**:\n",
    "- fix number of layers (e.g. 5);\n",
    "- choose type of the layers, non-linearity etc (e.g. `Dense(activation='relu')`);\n",
    "- optimize number of units in each layer;\n",
    "- introduce restrictions on number of parameters, either by:\n",
    "    - penalty e.g. `C * max(num_parameters - 1.0e+4, 0)`;\n",
    "    - scaling number of units until number of parameters drops to $10^4$;\n",
    "    - using fractions of total number of parameters for each layer (see `softmax` trick);\n",
    "- you may want to also include regularization coefficients (e.g. `alpha * l1 + beta * l2`).\n",
    "\n",
    "**Mad-data-scientist problem**:\n",
    "- try to optimize everything:\n",
    "    - find a way to encode variable number of layers (e.g. by introducing limit on number of layers);\n",
    "    - find a way to optimize activation function:\n",
    "        - e.g. by preselecting small set of activation functions and taking linear combination of them;\n",
    "        - skopt alse can deal with descrite variables (please, refer to the documentation of `dimensions` argument for optimizer);\n",
    "        - alternatively, you can introduce coeficients for each activation function (select 3-4) for each layers and split your layers according to these coefficients: `Concat(Dense(total_units * coef[0], activation='relu'), Dense(total_units * coef[1], activation='sigmoid'))`.\n",
    "\n",
    "**You can evaluate on the test set only once!**\n",
    "\n",
    "Tips:\n",
    "- try to optimize 1 parameter first, e.g. to select an appropriate kernel for GP, estimate noise level;\n",
    "-  `sigmoid` and `tanh` does not differ much (there is an easy-to-prove procedure that replaces one with another w/o changing the network);\n",
    "- if your want to have parameters that sum to one use `softmax` and fix one of the parameters to `1` (to remove overparametrization);\n",
    "- BO does not handle very well ordered descrete variables, it might be better to use `int(param)`:\n",
    "    - you might want to use cache in this case.\n",
    "- use small portion of the dataset e.g. `10^4` examples to accelerate search;\n",
    "- if you have symmetries in your parameterization, i.e. N parameter values correspond to the same network, you can report to BO all N parameters with the same value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### don't forget about others!\n",
    "\n",
    "import tensorflow as tf\n",
    "gpu_options = tf.GPUOptions(allow_growth=True, per_process_gpu_memory_fraction=0.1)\n",
    "tf_session = tf.InteractiveSession(config=tf.ConfigProto(gpu_options=gpu_options))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "\n",
    "keras.backend.tensorflow_backend.set_session(tf_session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is next?\n",
    "\n",
    "- you can experiment with acqusition functions:\n",
    "    - compare expected improvement (`EI`) and lower confidence bound (`LCB`) to probability of improvement (`PI`);\n",
    "- explore different regressions available https://scikit-optimize.github.io/learning/index.html\n",
    "- try to shoot yourself in the foot by perfroming naive Bayesian inference on a Neural Network.\n",
    "- try partial Bayesian Inference with Neural Networks: https://arxiv.org/pdf/1502.05700.pdf\n",
    "- try REMBO-ing your optimum value: https://arxiv.org/abs/1301.1942\n",
    "\n",
    "<img src=\"https://imgix.ranker.com/user_node_img/50066/1001300261/original/there-was-a-big-crew-turnover-in-rambo-iii-photo-u1?w=650&q=50&fm=jpg&fit=crop&crop=faces\" width=454 height=193>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
